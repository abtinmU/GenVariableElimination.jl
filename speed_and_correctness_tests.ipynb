{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkjfYzwzXL0"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRstxIs-jYqq"
      },
      "outputs": [],
      "source": [
        "using Pkg\n",
        "Pkg.activate(\".\");  Pkg.resolve()\n",
        "\n",
        "Pkg.add([\n",
        "    \"Gen\",\n",
        "    \"PyCall\",\n",
        "    \"FunctionalCollections\",\n",
        "    \"StatsFuns\",\n",
        "    \"BenchmarkTools\",\n",
        "    \"Test\",\n",
        "    \"Printf\"\n",
        "])\n",
        "ENV[\"PYTHON\"] = \"/usr/bin/python3\"\n",
        "Pkg.build(\"PyCall\")\n",
        "using PyCall\n",
        "run(`python3 -m pip install -U opt_einsum cupy-cuda12x`)\n",
        "\n",
        "try\n",
        "    run(`python3 -m pip install -U \"jax[cpu]\"`)\n",
        "catch e\n",
        "    @warn \"JAX install (CPU) failed; continuing with NumPy/CuPy only\" exception=e\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05O0sSqeKkSB"
      },
      "outputs": [],
      "source": [
        "using Pkg\n",
        "\n",
        "Pkg.activate(\"gve-test\"; shared=false)\n",
        "Pkg.add([\"Gen\", \"BenchmarkTools\"])\n",
        "Pkg.add(PackageSpec(name=\"Gen\", version=\"0.4.8\"))\n",
        "Pkg.add(PackageSpec(\n",
        "    url = \"https://github.com/abtinmU/GenVariableElimination.jl.git\",\n",
        "    rev = \"feature/einsum-jax\",\n",
        "))\n",
        "Pkg.precompile()\n",
        "\n",
        "Pkg.status(\"GenVariableElimination\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kmswe0hLks0i",
        "outputId": "234ff13d-d598-4fba-f45e-f33e1e566dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `/content/gve-test/Project.toml`\n",
            "  \u001b[90m[43085b26] \u001b[39mGenVariableElimination v0.2.0 `https://github.com/abtinmU/GenVariableElimination.jl.git#feature/einsum-jax`\n"
          ]
        }
      ],
      "source": [
        "using GenVariableElimination\n",
        "pathof(GenVariableElimination)\n",
        "Pkg.status(\"GenVariableElimination\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5254UVTEvfy"
      },
      "source": [
        "# correctness tests\n",
        "These tests check the correctness of the internal data structures. Similar tests were already implemented for the original GenVariableElimination.jl. I adapted them to check the correctness of the new method while comparing them with the original native method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDV2EotlEx4K",
        "outputId": "bb35732c-1aa4-49ba-9daf-99747ddaa597"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m`Gen.@load_generated_functions` is no longer necessary and will be removed in a future release.\n",
            "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Gen ~/.julia/packages/Gen/mP0Sq/src/Gen.jl:33\u001b[39m\n",
            "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m`Gen.@load_generated_functions` is no longer necessary and will be removed in a future release.\n",
            "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Gen ~/.julia/packages/Gen/mP0Sq/src/Gen.jl:33\u001b[39m\n",
            "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m`Gen.@load_generated_functions` is no longer necessary and will be removed in a future release.\n",
            "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Gen ~/.julia/packages/Gen/mP0Sq/src/Gen.jl:33\u001b[39m\n",
            "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m`Gen.@load_generated_functions` is no longer necessary and will be removed in a future release.\n",
            "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Gen ~/.julia/packages/Gen/mP0Sq/src/Gen.jl:33\u001b[39m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test results (native vs einsum):\n",
            "- compiling factor graph from trace  native=PASS (  0.0006 s)   einsum=PASS (  0.0006 s)   speedup=  1.01x\n",
            "- variable elimination          native=PASS (  0.0008 s)   einsum=PASS (  0.0030 s)   speedup=  0.28x\n",
            "- conditional_dist              native=PASS (  0.0007 s)   einsum=PASS (  0.0007 s)   speedup=  1.02x\n",
            "- MH always accepts (DML)       native=PASS (  0.0408 s)   einsum=PASS (  0.0424 s)   speedup=  0.96x\n",
            "- HMM FFBS                      native=PASS (  0.1310 s)   einsum=PASS (  0.5530 s)   speedup=  0.24x\n",
            "- SML basic block               native=PASS (  4.9426 s)   einsum=PASS (  6.0255 s)   speedup=  0.82x\n",
            "- Map                           native=PASS (  0.4994 s)   einsum=PASS (  2.2621 s)   speedup=  0.22x\n",
            "- Unfold and fixed-trace        native=PASS (  1.6553 s)   einsum=PASS (  4.5736 s)   speedup=  0.36x\n",
            "- DML public sampler accepts    native=PASS (  0.2282 s)   einsum=PASS (  1.0802 s)   speedup=  0.21x\n",
            "- SML public sampler accepts    native=PASS (  2.7221 s)   einsum=PASS (  3.5179 s)   speedup=  0.77x\n",
            "\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "##############################\n",
        "# Compares engines  :native vs :einsum\n",
        "##############################\n",
        "\n",
        "using Gen\n",
        "using Test\n",
        "using Printf\n",
        "using GenVariableElimination\n",
        "const M = GenVariableElimination\n",
        "\n",
        "@gen function backwards_sampler_dml_positional(\n",
        "    trace,\n",
        "    addrs,\n",
        "    latents::Dict{Any,M.Latent},\n",
        "    observations::Dict{Any,M.Observation},\n",
        "    engine::Union{Symbol,String}\n",
        ")\n",
        "    s = M.generate_backwards_sampler_fixed_trace(\n",
        "        trace, addrs, latents, observations; engine=Symbol(engine)\n",
        "    )\n",
        "    {*} ~ s()\n",
        "end\n",
        "\n",
        "# -------- utilities --------\n",
        "normed(arr) = arr / sum(arr)\n",
        "pretty(b) = b ? \"PASS\" : \"FAIL\"\n",
        "\n",
        "# call VE with or without engine keyword\n",
        "function call_variable_elimination(fg, order; engine=:native, backend=:auto, optimize=\"auto\", dtype=:float64, jit=true, cache=true)\n",
        "    return M.variable_elimination(fg, order; engine=engine, backend=backend, optimize=optimize, dtype=dtype, jit=jit, cache=cache)\n",
        "end\n",
        "\n",
        "# route elimination by engine (uses eliminate_einsum for einsum)\n",
        "function call_eliminate(fg, addr; engine=:native, backend=:auto, optimize=\"auto\", dtype=:float64, jit=true, cache=true)\n",
        "    if engine == :einsum\n",
        "        return M.eliminate_einsum(fg, addr; backend=backend, optimize=optimize, dtype=dtype, jit=jit, cache=cache)\n",
        "    else\n",
        "        return M.eliminate(fg, addr)\n",
        "    end\n",
        "end\n",
        "\n",
        "# make engine-aware samplers\n",
        "function call_generate_fixed_trace(trace, addrs; engine=:native, backend=:auto, optimize=\"auto\", dtype=:float64, jit=true, cache=true, latents=nothing, observations=nothing)\n",
        "    if latents === nothing\n",
        "        return M.generate_backwards_sampler_fixed_trace(trace, addrs; engine=engine, backend=backend, optimize=optimize, dtype=dtype, jit=jit, cache=cache)\n",
        "    else\n",
        "        return M.generate_backwards_sampler_fixed_trace(trace, addrs, latents, observations; engine=engine, backend=backend, optimize=optimize, dtype=dtype, jit=jit, cache=cache)\n",
        "    end\n",
        "end\n",
        "\n",
        "function call_generate_fixed_structure(trace, addrs; engine=:native, backend=:auto, optimize=\"auto\", dtype=:float64, jit=true, cache=true)\n",
        "    return M.generate_backwards_sampler_fixed_structure(trace, addrs; engine=engine, backend=backend, optimize=optimize, dtype=dtype, jit=jit, cache=cache)\n",
        "end\n",
        "\n",
        "#  tests\n",
        "\n",
        "function test_compiling_factor_graph()::Bool\n",
        "    @gen function foo()\n",
        "        x ~ bernoulli(0.6)\n",
        "        y ~ bernoulli(x ? 0.2 : 0.9)\n",
        "        z ~ bernoulli((x && y) ? 0.4 : 0.9)\n",
        "        w ~ bernoulli(z ? 0.4 : 0.5)\n",
        "    end\n",
        "    trace = Gen.simulate(foo, ())\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[:x] = M.Latent([true,false], [])\n",
        "    latents[:y] = M.Latent([true,false], [:x])\n",
        "    latents[:z] = M.Latent([true,false], [:x,:y])\n",
        "    latents[:w] = M.Latent([true,false], [:z])\n",
        "    observations = Dict{Any,Observation}()\n",
        "    fg = M.compile_trace_to_factor_graph(trace, latents, observations)\n",
        "\n",
        "    ok_nodes = (fg.num_factors == 4) && (length(fg.var_nodes) == 4)\n",
        "    function test_node(addr)\n",
        "        node = M.idx_to_var_node(fg, M.addr_to_idx(fg, addr))\n",
        "        return node.addr == addr &&\n",
        "               M.num_values(node) == 2 &&\n",
        "               M.idx_to_value(node, 1) == true &&\n",
        "               M.idx_to_value(node, 2) == false &&\n",
        "               M.value_to_idx(node, true) == 1 &&\n",
        "               M.value_to_idx(node, false) == 2\n",
        "    end\n",
        "    ok_nodes &= all(test_node.([:x,:y,:z,:w]))\n",
        "\n",
        "    all_factors = Set{typeof(first(collect(first(values(fg.var_nodes)).factor_nodes)))}()\n",
        "    for node in values(fg.var_nodes); union!(all_factors, M.factor_nodes(node)); end\n",
        "    ok_nfactors = length(all_factors) == 4\n",
        "\n",
        "    # f1\n",
        "    f1 = first(filter(fn -> (length(M.vars(fn))==1 && M.addr_to_idx(fg,:x) in M.vars(fn)), all_factors))\n",
        "    f1_xtrue  = M.factor_value(fg, f1, Dict(M.addr_to_idx(fg,:x)=>true))\n",
        "    f1_xfalse = M.factor_value(fg, f1, Dict(M.addr_to_idx(fg,:x)=>false))\n",
        "    ok_f1 = isapprox(normed([f1_xtrue,f1_xfalse]), normed([0.6,0.4]))\n",
        "\n",
        "    # f2\n",
        "    f2 = first(filter(fn -> (length(M.vars(fn))==2 && M.addr_to_idx(fg,:x) in M.vars(fn) && M.addr_to_idx(fg,:y) in M.vars(fn)), all_factors))\n",
        "    F2 = [\n",
        "        M.factor_value(fg, f2, Dict(M.addr_to_idx(fg,:x)=>true,  M.addr_to_idx(fg,:y)=>true)),\n",
        "        M.factor_value(fg, f2, Dict(M.addr_to_idx(fg,:x)=>true,  M.addr_to_idx(fg,:y)=>false)),\n",
        "        M.factor_value(fg, f2, Dict(M.addr_to_idx(fg,:x)=>false, M.addr_to_idx(fg,:y)=>true)),\n",
        "        M.factor_value(fg, f2, Dict(M.addr_to_idx(fg,:x)=>false, M.addr_to_idx(fg,:y)=>false))\n",
        "    ]\n",
        "    ok_f2 = isapprox(normed(F2), normed([0.2,0.8,0.9,0.1]))\n",
        "\n",
        "    # f3\n",
        "    f3 = first(filter(fn -> (length(M.vars(fn))==3 &&\n",
        "                             M.addr_to_idx(fg,:x) in M.vars(fn) &&\n",
        "                             M.addr_to_idx(fg,:y) in M.vars(fn) &&\n",
        "                             M.addr_to_idx(fg,:z) in M.vars(fn)), all_factors))\n",
        "    F3 = [\n",
        "        M.factor_value(fg, f3, Dict(M.addr_to_idx(fg,:x)=>true,  M.addr_to_idx(fg,:y)=>true,  M.addr_to_idx(fg,:z)=>true)),\n",
        "        M.factor_value(fg, f3, Dict(M.addr_to_idx(fg,:x)=>true,  M.addr_to_idx(fg,:y)=>false, M.addr_to_idx(fg,:z)=>true)),\n",
        "        M.factor_value(fg, f3, Dict(M.addr_to_idx(fg,:x)=>false, M.addr_to_idx(fg,:y)=>true,  M.addr_to_idx(fg,:z)=>true)),\n",
        "        M.factor_value(fg, f3, Dict(M.addr_to_idx(fg,:x)=>false, M.addr_to_idx(fg,:y)=>false, M.addr_to_idx(fg,:z)=>true)),\n",
        "        M.factor_value(fg, f3, Dict(M.addr_to_idx(fg,:x)=>true,  M.addr_to_idx(fg,:y)=>true,  M.addr_to_idx(fg,:z)=>false)),\n",
        "        M.factor_value(fg, f3, Dict(M.addr_to_idx(fg,:x)=>true,  M.addr_to_idx(fg,:y)=>false, M.addr_to_idx(fg,:z)=>false)),\n",
        "        M.factor_value(fg, f3, Dict(M.addr_to_idx(fg,:x)=>false, M.addr_to_idx(fg,:y)=>true,  M.addr_to_idx(fg,:z)=>false)),\n",
        "        M.factor_value(fg, f3, Dict(M.addr_to_idx(fg,:x)=>false, M.addr_to_idx(fg,:y)=>false, M.addr_to_idx(fg,:z)=>false))\n",
        "    ]\n",
        "    ok_f3 = isapprox(normed(F3), normed([0.4,0.9,0.9,0.9, 0.6,0.1,0.1,0.1]))\n",
        "\n",
        "    # f4\n",
        "    f4 = first(filter(fn -> (length(M.vars(fn))==2 &&\n",
        "                             M.addr_to_idx(fg,:z) in M.vars(fn) &&\n",
        "                             M.addr_to_idx(fg,:w) in M.vars(fn)), all_factors))\n",
        "    F4 = [\n",
        "        M.factor_value(fg, f4, Dict(M.addr_to_idx(fg,:z)=>true,  M.addr_to_idx(fg,:w)=>true)),\n",
        "        M.factor_value(fg, f4, Dict(M.addr_to_idx(fg,:z)=>true,  M.addr_to_idx(fg,:w)=>false)),\n",
        "        M.factor_value(fg, f4, Dict(M.addr_to_idx(fg,:z)=>false, M.addr_to_idx(fg,:w)=>true)),\n",
        "        M.factor_value(fg, f4, Dict(M.addr_to_idx(fg,:z)=>false, M.addr_to_idx(fg,:w)=>false))\n",
        "    ]\n",
        "    ok_f4 = isapprox(normed(F4), normed([0.4,0.6,0.5,0.5]))\n",
        "\n",
        "    return ok_nodes && ok_nfactors && ok_f1 && ok_f2 && ok_f3 && ok_f4\n",
        "end\n",
        "\n",
        "function test_variable_elimination(; engine=:native)::Bool\n",
        "    @gen function foo()\n",
        "        x ~ bernoulli(0.6)\n",
        "        y ~ bernoulli(x ? 0.2 : 0.9)\n",
        "        z ~ bernoulli((x && y) ? 0.4 : 0.9)\n",
        "        w ~ bernoulli(z ? 0.4 : 0.5)\n",
        "    end\n",
        "    trace = Gen.simulate(foo, ())\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[:x] = M.Latent([true,false], [])\n",
        "    latents[:y] = M.Latent([true,false], [:x])\n",
        "    latents[:z] = M.Latent([true,false], [:x,:y])\n",
        "    latents[:w] = M.Latent([true,false], [:z])\n",
        "    observations = Dict{Any,Observation}()\n",
        "    fg = M.compile_trace_to_factor_graph(trace, latents, observations)\n",
        "\n",
        "    fg1 = call_eliminate(fg, :w; engine=engine)  # -> f5\n",
        "    ok1 = (fg1.num_factors == 5) && (length(fg1.var_nodes) == 3)\n",
        "\n",
        "    all_factors1 = Set{typeof(first(collect(first(values(fg1.var_nodes)).factor_nodes)))}()\n",
        "    for node in values(fg1.var_nodes); union!(all_factors1, M.factor_nodes(node)); end\n",
        "    f5 = first(filter(fn -> (length(M.vars(fn))==1 && M.addr_to_idx(fg1,:z) in M.vars(fn)), all_factors1))\n",
        "    F5 = [ M.factor_value(fg1, f5, Dict(M.addr_to_idx(fg1,:z)=>true)),\n",
        "           M.factor_value(fg1, f5, Dict(M.addr_to_idx(fg1,:z)=>false)) ]\n",
        "    ok_f5 = isapprox(normed(F5), normed([0.4+0.6, 0.5+0.5]))\n",
        "\n",
        "    fg2 = call_eliminate(fg1, :x; engine=engine) # -> f6\n",
        "    ok2 = (fg2.num_factors == 6) && (length(fg2.var_nodes) == 2)\n",
        "    all_factors2 = Set{typeof(first(collect(first(values(fg2.var_nodes)).factor_nodes)))}()\n",
        "    for node in values(fg2.var_nodes); union!(all_factors2, M.factor_nodes(node)); end\n",
        "    f6 = first(filter(fn -> (length(M.vars(fn))==2 && M.addr_to_idx(fg2,:y) in M.vars(fn) && M.addr_to_idx(fg2,:z) in M.vars(fn)), all_factors2))\n",
        "    F6_true_true  = (0.6 * 0.2 * 0.4) + (0.4 * 0.9 * 0.9)\n",
        "    F6_true_false = (0.6 * 0.2 * 0.6) + (0.4 * 0.9 * 0.1)\n",
        "    F6_false_true = (0.6 * 0.8 * 0.9) + (0.4 * 0.1 * 0.9)\n",
        "    F6_false_false= (0.6 * 0.8 * 0.1) + (0.4 * 0.1 * 0.1)\n",
        "    F6 = [\n",
        "        M.factor_value(fg2, f6, Dict(M.addr_to_idx(fg2,:y)=>true,  M.addr_to_idx(fg2,:z)=>true)),\n",
        "        M.factor_value(fg2, f6, Dict(M.addr_to_idx(fg2,:y)=>true,  M.addr_to_idx(fg2,:z)=>false)),\n",
        "        M.factor_value(fg2, f6, Dict(M.addr_to_idx(fg2,:y)=>false, M.addr_to_idx(fg2,:z)=>true)),\n",
        "        M.factor_value(fg2, f6, Dict(M.addr_to_idx(fg2,:y)=>false, M.addr_to_idx(fg2,:z)=>false))\n",
        "    ]\n",
        "    ok_f6 = isapprox(normed(F6), normed([F6_true_true, F6_true_false, F6_false_true, F6_false_false]))\n",
        "    return ok1 && ok_f5 && ok2 && ok_f6\n",
        "end\n",
        "\n",
        "function test_conditional_dist()::Bool\n",
        "    @gen function foo()\n",
        "        x ~ bernoulli(0.6)\n",
        "        y ~ bernoulli(x ? 0.2 : 0.9)\n",
        "        z ~ bernoulli((x && y) ? 0.4 : 0.9)\n",
        "        w ~ bernoulli(z ? 0.4 : 0.5)\n",
        "    end\n",
        "    trace = Gen.simulate(foo, ())\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[:x] = M.Latent([true,false], [])\n",
        "    latents[:y] = M.Latent([true,false], [:x])\n",
        "    latents[:z] = M.Latent([true,false], [:x,:y])\n",
        "    latents[:w] = M.Latent([true,false], [:z])\n",
        "    observations = Dict{Any,Observation}()\n",
        "    fg = M.compile_trace_to_factor_graph(trace, latents, observations)\n",
        "\n",
        "    fg = M.eliminate(fg, :w)\n",
        "    fg = M.eliminate(fg, :x)\n",
        "\n",
        "    F6 = [ (0.6 * 0.2 * 0.4) + (0.4 * 0.9 * 0.9),\n",
        "           (0.6 * 0.2 * 0.6) + (0.4 * 0.9 * 0.1),\n",
        "           (0.6 * 0.8 * 0.9) + (0.4 * 0.1 * 0.9),\n",
        "           (0.6 * 0.8 * 0.1) + (0.4 * 0.1 * 0.1) ]\n",
        "    F5 = [0.4+0.6, 0.5+0.5]\n",
        "\n",
        "    values = Vector{Any}(undef, 4)\n",
        "    values[M.addr_to_idx(fg, :y)] = true\n",
        "    actual1 = M.conditional_dist(fg, values, :z)\n",
        "    expected1 = normed([F6[1]*F5[1], F6[2]*F5[2]])\n",
        "    values[M.addr_to_idx(fg, :y)] = false\n",
        "    actual2 = M.conditional_dist(fg, values, :z)\n",
        "    expected2 = normed([F6[3]*F5[1], F6[4]*F5[2]])\n",
        "    return isapprox(actual1, expected1) && isapprox(actual2, expected2)\n",
        "end\n",
        "\n",
        "function test_mh_always_accepts_dml(; engine=:native)::Bool\n",
        "    @gen function bar()\n",
        "        x ~ bernoulli(0.6)\n",
        "        y ~ bernoulli(x ? 0.2 : 0.9)\n",
        "        z ~ bernoulli((x && y) ? 0.4 : 0.9)\n",
        "        w ~ bernoulli(z ? 0.4 : 0.5)\n",
        "        obs ~ bernoulli((x && w) ? 0.4 : 0.1)\n",
        "    end\n",
        "    trace = Gen.simulate(bar, ())\n",
        "\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[:x] = M.Latent([true,false], [])\n",
        "    latents[:y] = M.Latent([true,false], [:x])\n",
        "    latents[:z] = M.Latent([true,false], [:x,:y])\n",
        "    latents[:w] = M.Latent([true,false], [:z])\n",
        "    observations = Dict{Any,Observation}()\n",
        "    observations[:obs] = M.Observation([:x,:w])\n",
        "\n",
        "    order = [:w,:x,:z,:y]\n",
        "    sampler = call_generate_fixed_trace(\n",
        "        trace, order;\n",
        "        engine=engine,\n",
        "        backend=:numpy,\n",
        "        dtype=:float64,\n",
        "        latents=latents,\n",
        "        observations=observations\n",
        "    )\n",
        "\n",
        "    ok = true\n",
        "    t = trace\n",
        "    for _ in 1:100\n",
        "        qf = Gen.simulate(sampler, ())\n",
        "        new_t, weight, _, discard = Gen.update(t, Gen.get_args(t), map(_->Gen.NoChange(), Gen.get_args(t)), Gen.get_choices(qf))\n",
        "        qb, _ = Gen.generate(sampler, (), discard)\n",
        "        log_ratio = weight + Gen.get_score(qb) - Gen.get_score(qf)\n",
        "        local tol = engine == :einsum ? 1e-10 : 1e-10\n",
        "        ok = ok && isfinite(log_ratio) && abs(log_ratio) <= tol\n",
        "        t = new_t\n",
        "    end\n",
        "    return ok\n",
        "end\n",
        "\n",
        "function test_hmm_ffbs(; engine=:native)::Bool\n",
        "    prior = rand(3); prior ./= sum(prior)\n",
        "    A = rand(3,3);  A ./= sum(A, dims=2)\n",
        "    B = rand(3,3);  B ./= sum(B, dims=2)\n",
        "    T = 10\n",
        "    @gen function hmm()\n",
        "        z = ({(:z,1)} ~ categorical(prior))\n",
        "        {(:x,1)} ~ categorical(B[z,:])\n",
        "        for t in 2:T\n",
        "            z = ({(:z,t)} ~ categorical(A[z,:]))\n",
        "            {(:x,t)} ~ categorical(B[z,:])\n",
        "        end\n",
        "    end\n",
        "\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[(:z,1)] = M.Latent(collect(1:3), [])\n",
        "    for t in 2:T\n",
        "        latents[(:z,t)] = M.Latent(collect(1:3), [(:z,t-1)])\n",
        "    end\n",
        "    observations = Dict{Any,Observation}()\n",
        "    for t in 1:T; observations[(:x,t)] = M.Observation([(:z,t)]); end\n",
        "    order = Any[(:z,t) for t in 1:T]\n",
        "\n",
        "    trace = Gen.simulate(hmm, ())\n",
        "    ok = true\n",
        "\n",
        "    let elim_order = order, lats = latents, obs = observations, eng = engine\n",
        "        @gen function dml_mh_sampler(tr)\n",
        "            {*} ~ backwards_sampler_dml_positional(tr, elim_order, lats, obs, eng)\n",
        "       end\n",
        "        for _ in 1:10\n",
        "            trace, acc = Gen.mh(trace, dml_mh_sampler, ())\n",
        "            ok &= acc\n",
        "        end\n",
        "    end\n",
        "    return ok\n",
        "end\n",
        "\n",
        "function compute_next_probs(x1)\n",
        "    probs = fill(1/20, 20)\n",
        "    probs[x1] *= 10\n",
        "    return probs ./ sum(probs)\n",
        "end\n",
        "@gen (static) function static_model()\n",
        "    p1 ~ beta(0.5, 0.5)\n",
        "    p2 ~ beta(0.5, 0.5)\n",
        "    p3 ~ beta(0.5, 0.5)\n",
        "    x1 ~ uniform_discrete(1, 20)\n",
        "    x2 ~ categorical(compute_next_probs(x1))\n",
        "    x3 ~ categorical(compute_next_probs(x2))\n",
        "    x4 ~ categorical(compute_next_probs(x3))\n",
        "    x5 ~ categorical(compute_next_probs(x4))\n",
        "    x6 ~ categorical(compute_next_probs(x5))\n",
        "    x7 ~ bernoulli(x6 > 5 ? p2 : p3)\n",
        "    x8 ~ bernoulli(x7 ? p2 : p3)\n",
        "    x9 ~ bernoulli(x8 ? p2 : p3)\n",
        "    x10 ~ bernoulli(x9 ? p2 : p3)\n",
        "    x11 ~ bernoulli(x10 ? p2 : p3)\n",
        "    x12 ~ bernoulli(x11 ? p2 : p3)\n",
        "    x13 ~ bernoulli(x12 ? p2 : p3)\n",
        "    x14 ~ bernoulli(x13 ? p2 : p3)\n",
        "    x15 ~ bernoulli(x14 ? p2 : p3)\n",
        "    x16 ~ bernoulli(x15 ? p2 : p3)\n",
        "    x17 ~ bernoulli(x16 ? p2 : p3)\n",
        "    x18 ~ bernoulli(x17 ? p2 : p3)\n",
        "    x19 ~ bernoulli(x18 ? p2 : p3)\n",
        "    x20 ~ bernoulli(x19 ? 0.5 : 0.1)\n",
        "end\n",
        "function test_sml_basic_block(; engine=:native)::Bool\n",
        "    @load_generated_functions()\n",
        "    trace = Gen.simulate(static_model, ())\n",
        "\n",
        "    # structure-specialized sampler\n",
        "    sampler = call_generate_fixed_structure(trace, [:x1,:x2,:x3]; engine=engine)\n",
        "    ok = true\n",
        "    for _ in 1:100\n",
        "        trace, acc = Gen.mh(trace, sampler, ())\n",
        "        ok &= acc\n",
        "    end\n",
        "\n",
        "    for _ in 1:100\n",
        "        qf = Gen.simulate(sampler, (trace,))\n",
        "        new_t, weight, _, discard =\n",
        "            Gen.update(trace, Gen.get_args(trace),\n",
        "                      map(_->Gen.NoChange(), Gen.get_args(trace)),\n",
        "                      Gen.get_choices(qf))\n",
        "        qb, _ = Gen.generate(sampler, (new_t,), discard)\n",
        "\n",
        "        # tolerance tuned for numeric backend/dtype\n",
        "        tol = 1e-10\n",
        "        ok &= (abs(weight + Gen.get_score(qb) - Gen.get_score(qf)) < tol)\n",
        "\n",
        "        trace = new_t\n",
        "    end\n",
        "\n",
        "    return ok\n",
        "end\n",
        "\n",
        "@gen (static) function local_model(inlier_mean)\n",
        "    z ~ bernoulli(0.5)\n",
        "    x ~ normal(z ? inlier_mean : 0.0, z ? 1.0 : 10.0)\n",
        "end\n",
        "@gen (static) function global_model(n)\n",
        "    inlier_mean ~ normal(0, 10.0)\n",
        "    data ~ Map(local_model)(fill(inlier_mean, n))\n",
        "end\n",
        "function test_map(; engine=:native)::Bool\n",
        "    @load_generated_functions()\n",
        "    n = 10\n",
        "    trace = Gen.simulate(global_model, (n,))\n",
        "    sampler = call_generate_fixed_structure(trace, [(:data=>i=>:z) for i in 1:n]; engine=engine)\n",
        "    ok = true\n",
        "    for _ in 1:100\n",
        "        trace, acc = Gen.mh(trace, sampler, ())\n",
        "        ok &= acc\n",
        "        trace, _ = Gen.mh(trace, Gen.select(:inlier_mean))\n",
        "    end\n",
        "    return ok\n",
        "end\n",
        "\n",
        "\n",
        "prior = rand(3); prior ./= sum(prior)\n",
        "A = rand(3,3);  A ./= sum(A, dims=2)\n",
        "B = rand(3,3);  B ./= sum(B, dims=2)\n",
        "@gen (static) function step(t::Int, z_prev::Int)\n",
        "    z ~ categorical(A[z_prev,:])\n",
        "    x ~ categorical(B[z,:])\n",
        "    return z\n",
        "end\n",
        "@gen (static) function hmm(T::Int)\n",
        "    z_init ~ categorical(prior)\n",
        "    x_init ~ categorical(B[z_init,:])\n",
        "    steps ~ (Unfold(step))(T, z_init)\n",
        "end\n",
        "function test_unfold_and_fixed_trace(; engine=:native)::Bool\n",
        "\n",
        "    @load_generated_functions()\n",
        "\n",
        "    # Unfold acceptance\n",
        "    trace = Gen.simulate(hmm, (10,))\n",
        "    sampler = call_generate_fixed_structure(trace, [:z_init, ([:steps=>t=>:z for t in 1:10]...)]; engine=engine)\n",
        "    ok1 = true\n",
        "    for _ in 1:100\n",
        "        trace, acc = Gen.mh(trace, sampler, ())\n",
        "        ok1 &= acc\n",
        "    end\n",
        "\n",
        "    # fixed trace log-ml equality\n",
        "    Tlen = 10\n",
        "    trace2 = Gen.simulate(hmm, (Tlen,))\n",
        "    observations = Gen.choicemap()\n",
        "    observations[:x_init] = trace2[:x_init]\n",
        "    for t in 1:Tlen\n",
        "        observations[:steps=>t=>:x] = trace2[:steps=>t=>:x]\n",
        "    end\n",
        "    sampler2 = call_generate_fixed_trace(trace2, [:z_init, ([:steps=>t=>:z for t in 1:Tlen]...)]; engine=engine)\n",
        "    q1 = Gen.simulate(sampler2, ())\n",
        "    p1, = Gen.generate(hmm, (Tlen,), merge(observations, Gen.get_choices(q1)))\n",
        "    q2 = Gen.simulate(sampler2, ())\n",
        "    p2, = Gen.generate(hmm, (Tlen,), merge(observations, Gen.get_choices(q2)))\n",
        "    ok2 = isapprox(Gen.get_score(p1) - Gen.get_score(q1), Gen.get_score(p2) - Gen.get_score(q2))\n",
        "    return ok1 && ok2\n",
        "end\n",
        "\n",
        "#  run & print\n",
        "using BenchmarkTools, Printf\n",
        "\n",
        "using Statistics  # for mean and std\n",
        "\n",
        "# Pretty-print an array on one line\n",
        "fmtarr(x) = \"[\" * join(map(y -> @sprintf(\"%.6g\", y), vec(collect(x))), \", \") * \"]\"\n",
        "\n",
        "# Show factor variable order as symbolic addrs\n",
        "_var_order_labels(fg, f) = [M.idx_to_var_node(fg, vidx).addr for vidx in M.vars(f)]\n",
        "\n",
        "\n",
        "# One place to print diffs for a failing check\n",
        "function print_diff(name; expected, native, einsum)\n",
        "    println(\"Failure diagnostics: \", name)\n",
        "    println(\"  expected: \", fmtarr(expected))\n",
        "    println(\"  native:   \", fmtarr(native))\n",
        "    println(\"  einsum:   \", fmtarr(einsum))\n",
        "    diffe = abs.(einsum .- expected)\n",
        "    diffn = abs.(native .- expected)\n",
        "    println(\"  |einsum - expected|: \", fmtarr(diffe))\n",
        "    println(\"  |native - expected|: \", fmtarr(diffn))\n",
        "    println()\n",
        "end\n",
        "\n",
        "\n",
        "\n",
        "# Run a test and return (ok, seconds).\n",
        "# `bench=true` uses @belapsed for stable timings.\n",
        "run_timed(fn; bench::Bool=false, seconds::Float64=1.0) = begin\n",
        "    if bench\n",
        "        fn()  # warm-up\n",
        "        t = @belapsed $fn() seconds=seconds evals=1\n",
        "        ok = fn()\n",
        "        return ok, t\n",
        "    else\n",
        "        local tm = @timed ok = fn()\n",
        "        return ok, tm.time\n",
        "    end\n",
        "end\n",
        "\n",
        "\n",
        "# Rebuild the tiny Bernoulli graph used in several tests\n",
        "@gen function _foo_model()\n",
        "    x ~ bernoulli(0.6)\n",
        "    y ~ bernoulli(x ? 0.2 : 0.9)\n",
        "    z ~ bernoulli((x && y) ? 0.4 : 0.9)\n",
        "    w ~ bernoulli(z ? 0.4 : 0.5)\n",
        "end\n",
        "\n",
        "function _make_fg_for_foo()\n",
        "    trace = Gen.simulate(_foo_model, ())\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[:x] = M.Latent([true,false], [])\n",
        "    latents[:y] = M.Latent([true,false], [:x])\n",
        "    latents[:z] = M.Latent([true,false], [:x,:y])\n",
        "    latents[:w] = M.Latent([true,false], [:z])\n",
        "    observations = Dict{Any,Observation}()\n",
        "    return M.compile_trace_to_factor_graph(trace, latents, observations)\n",
        "end\n",
        "\n",
        "# diagnostics: variable elimination\n",
        "function diagnose_variable_elimination()\n",
        "    fg0 = _make_fg_for_foo()\n",
        "\n",
        "    # expected values used in the test\n",
        "    F5_expected = [0.4 + 0.6, 0.5 + 0.5]\n",
        "    F6_true_true  = (0.6 * 0.2 * 0.4) + (0.4 * 0.9 * 0.9)\n",
        "    F6_true_false = (0.6 * 0.2 * 0.6) + (0.4 * 0.9 * 0.1)\n",
        "    F6_false_true = (0.6 * 0.8 * 0.9) + (0.4 * 0.1 * 0.9)\n",
        "    F6_false_false= (0.6 * 0.8 * 0.1) + (0.4 * 0.1 * 0.1)\n",
        "    F6_expected = [F6_true_true, F6_true_false, F6_false_true, F6_false_false]\n",
        "\n",
        "    # helper to pull factor arrays\n",
        "    function f5_f6_for(engine_sym)\n",
        "        fg1 = call_eliminate(fg0, :w; engine=engine_sym)  # f5 is a unary factor on z\n",
        "        all_factors1 = Set{typeof(first(collect(first(values(fg1.var_nodes)).factor_nodes)))}()\n",
        "        for node in values(fg1.var_nodes); union!(all_factors1, M.factor_nodes(node)); end\n",
        "        f5 = first(filter(fn -> (length(M.vars(fn))==1 && M.addr_to_idx(fg1,:z) in M.vars(fn)), all_factors1))\n",
        "        F5 = [\n",
        "            M.factor_value(fg1, f5, Dict(M.addr_to_idx(fg1,:z)=>true)),\n",
        "            M.factor_value(fg1, f5, Dict(M.addr_to_idx(fg1,:z)=>false))\n",
        "        ]\n",
        "\n",
        "        fg2 = call_eliminate(fg1, :x; engine=engine_sym)  # f6 is a binary factor on y,z\n",
        "        all_factors2 = Set{typeof(first(collect(first(values(fg2.var_nodes)).factor_nodes)))}()\n",
        "        for node in values(fg2.var_nodes); union!(all_factors2, M.factor_nodes(node)); end\n",
        "        f6 = first(filter(fn -> (length(M.vars(fn))==2 &&\n",
        "                                 M.addr_to_idx(fg2,:y) in M.vars(fn) &&\n",
        "                                 M.addr_to_idx(fg2,:z) in M.vars(fn)), all_factors2))\n",
        "        F6 = [\n",
        "            M.factor_value(fg2, f6, Dict(M.addr_to_idx(fg2,:y)=>true,  M.addr_to_idx(fg2,:z)=>true)),\n",
        "            M.factor_value(fg2, f6, Dict(M.addr_to_idx(fg2,:y)=>true,  M.addr_to_idx(fg2,:z)=>false)),\n",
        "            M.factor_value(fg2, f6, Dict(M.addr_to_idx(fg2,:y)=>false, M.addr_to_idx(fg2,:z)=>true)),\n",
        "            M.factor_value(fg2, f6, Dict(M.addr_to_idx(fg2,:y)=>false, M.addr_to_idx(fg2,:z)=>false))\n",
        "        ]\n",
        "        return F5, F6\n",
        "    end\n",
        "\n",
        "    F5_native, F6_native = f5_f6_for(:native)\n",
        "    F5_einsum, F6_einsum = f5_f6_for(:einsum)\n",
        "\n",
        "    print_diff(\"F5 over z after eliminating w\";\n",
        "        expected=normed(F5_expected),\n",
        "        native=normed(F5_native),\n",
        "        einsum=normed(F5_einsum))\n",
        "\n",
        "    print_diff(\"F6 over y,z after eliminating x\";\n",
        "        expected=normed(F6_expected),\n",
        "        native=normed(F6_native),\n",
        "        einsum=normed(F6_einsum))\n",
        "\n",
        "    # Show which var order each engine produced for F6\n",
        "    let\n",
        "        fg1n = call_eliminate(_make_fg_for_foo(), :w; engine=:native)\n",
        "        fg2n = call_eliminate(fg1n, :x; engine=:native)\n",
        "        all2n = Set{typeof(first(collect(first(values(fg2n.var_nodes)).factor_nodes)))}()\n",
        "        for node in values(fg2n.var_nodes); union!(all2n, M.factor_nodes(node)); end\n",
        "        f6n = first(filter(fn -> (length(M.vars(fn))==2 &&\n",
        "                                  M.addr_to_idx(fg2n,:y) in M.vars(fn) &&\n",
        "                                  M.addr_to_idx(fg2n,:z) in M.vars(fn)), all2n))\n",
        "        println(\"  native F6 var order: \", _var_order_labels(fg2n, f6n))\n",
        "    end\n",
        "    let\n",
        "        fg1e = call_eliminate(_make_fg_for_foo(), :w; engine=:einsum)\n",
        "        fg2e = call_eliminate(fg1e, :x; engine=:einsum)\n",
        "        all2e = Set{typeof(first(collect(first(values(fg2e.var_nodes)).factor_nodes)))}()\n",
        "        for node in values(fg2e.var_nodes); union!(all2e, M.factor_nodes(node)); end\n",
        "        f6e = first(filter(fn -> (length(M.vars(fn))==2 &&\n",
        "                                  M.addr_to_idx(fg2e,:y) in M.vars(fn) &&\n",
        "                                  M.addr_to_idx(fg2e,:z) in M.vars(fn)), all2e))\n",
        "        println(\"  einsum F6 var order: \", _var_order_labels(fg2e, f6e))\n",
        "    end\n",
        "\n",
        "end\n",
        "\n",
        "# diagnostics: conditional_dist\n",
        "function diagnose_conditional_dist()\n",
        "    # expected numbers as in the test\n",
        "    F6 = [ (0.6 * 0.2 * 0.4) + (0.4 * 0.9 * 0.9),\n",
        "           (0.6 * 0.2 * 0.6) + (0.4 * 0.9 * 0.1),\n",
        "           (0.6 * 0.8 * 0.9) + (0.4 * 0.1 * 0.9),\n",
        "           (0.6 * 0.8 * 0.1) + (0.4 * 0.1 * 0.1) ]\n",
        "    F5 = [0.4 + 0.6, 0.5 + 0.5]\n",
        "    expected_ytrue  = normed([F6[1]*F5[1], F6[2]*F5[2]])\n",
        "    expected_yfalse = normed([F6[3]*F5[1], F6[4]*F5[2]])\n",
        "\n",
        "    function cond_for(engine_sym)\n",
        "        fg = _make_fg_for_foo()\n",
        "        fg = call_eliminate(fg, :w; engine=engine_sym)\n",
        "        fg = call_eliminate(fg, :x; engine=engine_sym)\n",
        "\n",
        "        values = Vector{Any}(undef, 4)\n",
        "        values[M.addr_to_idx(fg, :y)] = true\n",
        "        a1 = M.conditional_dist(fg, values, :z)\n",
        "        values[M.addr_to_idx(fg, :y)] = false\n",
        "        a2 = M.conditional_dist(fg, values, :z)\n",
        "        return a1, a2\n",
        "    end\n",
        "\n",
        "    a1_native, a2_native = cond_for(:native)\n",
        "    a1_einsum, a2_einsum = cond_for(:einsum)\n",
        "\n",
        "    print_diff(\"P(z|y=true)\";\n",
        "        expected=expected_ytrue, native=a1_native, einsum=a1_einsum)\n",
        "    print_diff(\"P(z|y=false)\";\n",
        "        expected=expected_yfalse, native=a2_native, einsum=a2_einsum)\n",
        "end\n",
        "\n",
        "\n",
        "function diagnose_mh_always_accepts_dml()\n",
        "    @gen function bar()\n",
        "        x ~ bernoulli(0.6)\n",
        "        y ~ bernoulli(x ? 0.2 : 0.9)\n",
        "        z ~ bernoulli((x && y) ? 0.4 : 0.9)\n",
        "        w ~ bernoulli(z ? 0.4 : 0.5)\n",
        "        obs ~ bernoulli((x && w) ? 0.4 : 0.1)\n",
        "    end\n",
        "    trace0 = Gen.simulate(bar, ())\n",
        "\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[:x] = M.Latent([true,false], [])\n",
        "    latents[:y] = M.Latent([true,false], [:x])\n",
        "    latents[:z] = M.Latent([true,false], [:x,:y])\n",
        "    latents[:w] = M.Latent([true,false], [:z])\n",
        "    observations = Dict{Any,Observation}()\n",
        "    observations[:obs] = M.Observation([:x,:w])\n",
        "    order = [:w,:x,:z,:y]\n",
        "\n",
        "    function collect_ratios(engine_sym)\n",
        "        sampler = call_generate_fixed_trace(trace0, order; engine=engine_sym,\n",
        "                                            latents=latents, observations=observations)\n",
        "        t = trace0\n",
        "        ratios = Float64[]\n",
        "        triples = Tuple{Float64,Float64,Float64}[]\n",
        "        tol = (engine_sym == :einsum) ? 1e-10 : 1e-10\n",
        "        for _ in 1:100\n",
        "            qf = Gen.simulate(sampler, ())\n",
        "            new_t, weight, _, discard =\n",
        "                Gen.update(t, Gen.get_args(t), map(_->Gen.NoChange(), Gen.get_args(t)), Gen.get_choices(qf))\n",
        "            qb, _ = Gen.generate(sampler, (), discard)\n",
        "            lr = weight + Gen.get_score(qb) - Gen.get_score(qf)\n",
        "            push!(ratios, lr); push!(triples, (weight, Gen.get_score(qb), Gen.get_score(qf)))\n",
        "            t = new_t\n",
        "        end\n",
        "        return ratios, triples, tol\n",
        "    end\n",
        "\n",
        "    rn, tn, toln = collect_ratios(:native)\n",
        "    re, te, tole = collect_ratios(:einsum)\n",
        "\n",
        "    absn = abs.(rn); abse = abs.(re)\n",
        "    worst_e = sortperm(abse, rev=true)[1:min(end, 5)]\n",
        "    println(\"Failure diagnostics: MH always accepts (DML)\")\n",
        "    println(\"  tol_native=\", toln, \"  tol_einsum=\", tole)\n",
        "    println(\"  native max |log_ratio|: \", maximum(absn))\n",
        "    println(\"  einsum  max |log_ratio|: \", maximum(abse))\n",
        "    for i in worst_e\n",
        "        w, qb, qf = te[i]\n",
        "        println(\"  iter=\", i,\n",
        "                \"  einsum log_ratio=\", re[i],\n",
        "                \"  components: weight=\", w, \" score(qb)=\", qb, \" score(qf)=\", qf)\n",
        "    end\n",
        "    println()\n",
        "end\n",
        "\n",
        "\n",
        "# build per-trace samplers with engine baked in\n",
        "\n",
        "@gen function _sml_kernel(tr, addrs, eng)\n",
        "    s = M.generate_backwards_sampler_fixed_structure(\n",
        "            tr, addrs; engine = Symbol(eng))\n",
        "    {*} ~ s(tr)   # <-- pass the trace here\n",
        "end\n",
        "\n",
        "@gen function _dml_kernel(tr, addrs, latents, observations, eng)\n",
        "    s = M.generate_backwards_sampler_fixed_trace(\n",
        "            tr, addrs, latents, observations; engine = Symbol(eng))\n",
        "    {*} ~ s()     # fixed-trace variant takes no args\n",
        "end\n",
        "\n",
        "\n",
        "\n",
        "# Public DML sampler should MH-accept every step (engine-agnostic)\n",
        "\n",
        "function test_dml_public_sampler_accepts(; engine=:native)::Bool\n",
        "    @gen function bar()\n",
        "        x ~ bernoulli(0.6)\n",
        "        y ~ bernoulli(x ? 0.2 : 0.9)\n",
        "        z ~ bernoulli((x && y) ? 0.4 : 0.9)\n",
        "        w ~ bernoulli(z ? 0.4 : 0.5)\n",
        "        obs ~ bernoulli((x && w) ? 0.4 : 0.1)\n",
        "    end\n",
        "    trace = Gen.simulate(bar, ())\n",
        "\n",
        "    latents = Dict{Any,M.Latent}(\n",
        "        :x=>M.Latent([true,false], []),\n",
        "        :y=>M.Latent([true,false], [:x]),\n",
        "        :z=>M.Latent([true,false], [:x,:y]),\n",
        "        :w=>M.Latent([true,false], [:z]),\n",
        "    )\n",
        "    observations = Dict{Any,M.Observation}(:obs=>M.Observation([:x,:w]))\n",
        "    order = [:w,:x,:z,:y]\n",
        "\n",
        "    ok = true\n",
        "    for _ in 1:100\n",
        "        trace, acc = Gen.mh(trace, _dml_kernel, (order, latents, observations, engine))\n",
        "        ok &= acc\n",
        "    end\n",
        "    return ok\n",
        "end\n",
        "\n",
        "function test_sml_generic_kernel_public(; engine=:native)::Bool\n",
        "    @load_generated_functions()\n",
        "    trace = Gen.simulate(static_model, ())\n",
        "    ok = true\n",
        "    for _ in 1:100\n",
        "        trace, acc = mh(trace, _sml_kernel, ([:x1,:x2,:x3], engine))\n",
        "        ok &= acc\n",
        "    end\n",
        "    return ok\n",
        "end\n",
        "\n",
        "\n",
        "\n",
        "function run_suite(label; engine=:native, bench=false, seconds=1.0)\n",
        "    results = Dict{String,Bool}()\n",
        "    timings = Dict{String,Float64}()\n",
        "\n",
        "    # little helper\n",
        "    function run(name::String, fn)\n",
        "        ok, t = run_timed(fn; bench=bench, seconds=seconds)\n",
        "        results[name] = ok\n",
        "        timings[name] = t\n",
        "    end\n",
        "\n",
        "    run(\"compiling factor graph from trace\", () -> test_compiling_factor_graph())\n",
        "    run(\"variable elimination\",             () -> test_variable_elimination(engine=engine))\n",
        "    run(\"conditional_dist\",                 () -> test_conditional_dist())\n",
        "    run(\"MH always accepts (DML)\",          () -> test_mh_always_accepts_dml(engine=engine))\n",
        "    run(\"HMM FFBS\",                         () -> test_hmm_ffbs(engine=engine))\n",
        "    run(\"SML basic block\",                  () -> test_sml_basic_block(engine=engine))\n",
        "    run(\"Map\",                              () -> test_map(engine=engine))\n",
        "    run(\"Unfold and fixed-trace\",           () -> test_unfold_and_fixed_trace(engine=engine))\n",
        "    run(\"DML public sampler accepts\",       () -> test_dml_public_sampler_accepts(engine=engine))\n",
        "    run(\"SML public sampler accepts\",       () -> test_sml_generic_kernel_public(engine=engine))\n",
        "\n",
        "    return (label=label, results=results, timings=timings)\n",
        "end\n",
        "\n",
        "\n",
        "native_suite = run_suite(\"Native\"; engine=:native, bench=true, seconds=1.0)\n",
        "einsum_suite = run_suite(\"Einsum\"; engine=:einsum, bench=true, seconds=1.0)\n",
        "\n",
        "all_tests = [\n",
        "    \"compiling factor graph from trace\",\n",
        "    \"variable elimination\",\n",
        "    \"conditional_dist\",\n",
        "    \"MH always accepts (DML)\",\n",
        "    \"HMM FFBS\",\n",
        "    \"SML basic block\",\n",
        "    \"Map\",\n",
        "    \"Unfold and fixed-trace\",\n",
        "    \"DML public sampler accepts\",\n",
        "    \"SML public sampler accepts\"\n",
        "]\n",
        "\n",
        "println(\"Test results (native vs einsum):\")\n",
        "fmt = \"- %-28s  native=%-4s (%8.4f s)   einsum=%-4s (%8.4f s)   speedup=%6.2fx\\n\"\n",
        "for tname in all_tests\n",
        "    n  = native_suite.results[tname];  tn = native_suite.timings[tname]\n",
        "    e  = einsum_suite.results[tname];  te = einsum_suite.timings[tname]\n",
        "    sp = tn / te\n",
        "    Printf.format(stdout, Printf.Format(fmt),\n",
        "                  tname, n ? \"PASS\" : \"FAIL\", tn, e ? \"PASS\" : \"FAIL\", te, sp)\n",
        "end\n",
        "println()\n",
        "\n",
        "# Print diagnostics only for failures\n",
        "diagnosers = Dict(\n",
        "    \"variable elimination\" => diagnose_variable_elimination,\n",
        "    \"conditional_dist\"     => diagnose_conditional_dist,\n",
        "    \"MH always accepts (DML)\" => diagnose_mh_always_accepts_dml,\n",
        ")\n",
        "\n",
        "for tname in all_tests\n",
        "    n = native_suite.results[tname]\n",
        "    e = einsum_suite.results[tname]\n",
        "    if !(n && e)\n",
        "        println(\"\\nDetailed numbers for failed test: \", tname)\n",
        "        if haskey(diagnosers, tname)\n",
        "            diagnosers[tname]()\n",
        "        else\n",
        "            println(\"(no numeric diff printer wired for this test)\")\n",
        "        end\n",
        "    end\n",
        "end\n",
        "\n",
        "println(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# speed checks\n",
        "Now some speed comparisons between the native method and the einsum+jax method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT7qKPyffB0"
      },
      "source": [
        "## speed check 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmKyAl7vwdhX",
        "outputId": "b752e36e-f4b7-438a-ad60-fa6c305149d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HMM VE  K=16  T=50  |  native=89.295s   einsum(jax,float32)=0.494s   speedup=180.85x\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(t_native = 89.294777604, t_einsum = 0.493738768)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "using BenchmarkTools, Printf\n",
        "\n",
        "@gen function hmm_big(K::Int, T::Int, A, B, prior)\n",
        "    z = ({(:z,1)} ~ categorical(prior))\n",
        "    {(:x,1)} ~ categorical(B[z,:])\n",
        "    for t in 2:T\n",
        "        z = ({(:z,t)} ~ categorical(A[z,:]))\n",
        "        {(:x,t)} ~ categorical(B[z,:])\n",
        "    end\n",
        "end\n",
        "\n",
        "function bench_hmm_ve_big(; K::Int=128, T::Int=300,\n",
        "                           einsum_backend::Symbol=:jax,\n",
        "                           einsum_dtype::Symbol=:float32,\n",
        "                           optimize::Union{String,Symbol}=\"greedy\")\n",
        "\n",
        "    # random normalized params\n",
        "    prior = rand(K); prior ./= sum(prior)\n",
        "    A = rand(K,K);  A ./= sum(A, dims=2)\n",
        "    B = rand(K,K);  B ./= sum(B, dims=2)\n",
        "\n",
        "    # build one trace (observations live inside it)\n",
        "    tr = Gen.simulate(hmm_big, (K, T, A, B, prior))\n",
        "\n",
        "    # factor graph\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[(:z,1)] = Latent(collect(1:K), Any[])\n",
        "    for t in 2:T\n",
        "        latents[(:z,t)] = Latent(collect(1:K), Any[(:z,t-1)])\n",
        "    end\n",
        "    observations = Dict{Any,Observation}()\n",
        "    for t in 1:T\n",
        "        observations[(:x,t)] = Observation(Any[(:z,t)])\n",
        "    end\n",
        "\n",
        "    fg = compile_trace_to_factor_graph(tr, latents, observations)\n",
        "    order = Any[(:z,t) for t in 1:T]\n",
        "\n",
        "    # warm-up (JIT / planner / array caches)\n",
        "    variable_elimination(fg, order; engine=:native)\n",
        "    variable_elimination(fg, order; engine=:einsum,\n",
        "                         backend=einsum_backend,\n",
        "                         dtype=einsum_dtype,\n",
        "                         optimize=optimize,\n",
        "                         jit=true, cache=true)\n",
        "\n",
        "    # timings (fg is immutable; VE returns a new graph)\n",
        "    t_native = @belapsed variable_elimination($fg, $order; engine=:native)\n",
        "    t_einsum = @belapsed variable_elimination($fg, $order; engine=:einsum,\n",
        "                                              backend=$einsum_backend,\n",
        "                                              dtype=$einsum_dtype,\n",
        "                                              optimize=$optimize,\n",
        "                                              jit=true, cache=true)\n",
        "\n",
        "    println(@sprintf(\"HMM VE  K=%d  T=%d  |  native=%.3fs   einsum(%s,%s)=%.3fs   speedup=%.2fx\",\n",
        "                     K, T, t_native, String(einsum_backend), String(einsum_dtype), t_einsum, t_native/t_einsum))\n",
        "\n",
        "    return (t_native=t_native, t_einsum=t_einsum)\n",
        "end\n",
        "\n",
        "bench_hmm_ve_big(K=16,  T=50)           # warm-up-ish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2BvF1aK0KHu",
        "outputId": "7867ad65-84c9-4e37-fbb3-05b4572b85a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HMM VE  K=48  T=50  |  native=621.312s   einsum(jax,float32)=1.087s   speedup=571.72x\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(t_native = 621.312468637, t_einsum = 1.086737142)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bench_hmm_ve_big(K=48, T=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7OhIkaOysMB",
        "outputId": "9a9c4110-6833-4412-df2f-62c500e183d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HMM VE  K=62  T=50  |  native=1030.990s   einsum(jax,float32)=1.381s   speedup=746.44x\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(t_native = 1030.990331316, t_einsum = 1.381216314)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bench_hmm_ve_big(K=62, T=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_ZsL2S2fbCn"
      },
      "source": [
        "## speed check 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa-GhNwDJ5Nu",
        "outputId": "7a882b12-800a-4b4b-96d9-84c34f38d670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SML chain VE benchmark  (K=16, L=32, backend=auto, dtype=float32, jit=true)\n",
            "  native (CPU greedy):        8.0185 s\n",
            "  einsum (opt_einsum/JAX):    0.1048 s\n",
            "  speedup (native/einsum):    76.543x\n"
          ]
        }
      ],
      "source": [
        "using Random, BenchmarkTools\n",
        "\n",
        "# Global parameters used by the static @gen functions\n",
        "const CHAIN = Dict{Symbol,Any}()\n",
        "\n",
        "function setup_chain!(K::Int, L::Int; seed::Int=42)\n",
        "    rng = MersenneTwister(seed)\n",
        "    prior = rand(rng, K); prior ./= sum(prior)\n",
        "    A = rand(rng, K, K);  A ./= sum(A, dims=2)  # transitions\n",
        "    B = rand(rng, K, K);  B ./= sum(B, dims=2)  # emissions\n",
        "\n",
        "    CHAIN[:K] = K\n",
        "    CHAIN[:L] = L\n",
        "    CHAIN[:prior] = prior\n",
        "    CHAIN[:A] = A\n",
        "    CHAIN[:B] = B\n",
        "    return nothing\n",
        "end\n",
        "\n",
        "#  Static SML model (top-level!)\n",
        "@gen (static) function chain_step(t::Int, z_prev::Int)\n",
        "    z ~ categorical(CHAIN[:A][z_prev, :])\n",
        "    x ~ categorical(CHAIN[:B][z, :])\n",
        "    return z\n",
        "end\n",
        "\n",
        "@gen (static) function chain_model(L::Int)\n",
        "    z1 ~ categorical(CHAIN[:prior])\n",
        "    x1 ~ categorical(CHAIN[:B][z1, :])\n",
        "    zs ~ (Unfold(chain_step))(L-1, z1)\n",
        "end\n",
        "\n",
        "#  Factor graph + elimination order\n",
        "function make_fg_chain(K::Int, L::Int)\n",
        "    setup_chain!(K, L)\n",
        "    trace = Gen.simulate(chain_model, (L,))\n",
        "\n",
        "    # Latents: all z's in the chain\n",
        "    latents = Dict{Any,Latent}()\n",
        "    latents[:z1] = Latent(collect(1:CHAIN[:K]), Any[])\n",
        "    for t in 1:L-1\n",
        "        parent = (t == 1) ? :z1 : (:zs=>t-1=>:z)\n",
        "        latents[:zs=>t=>:z] = Latent(collect(1:CHAIN[:K]), Any[parent])\n",
        "    end\n",
        "\n",
        "    # Observations: each x depends on its z\n",
        "    observations = Dict{Any,Observation}()\n",
        "    observations[:x1] = Observation(Any[:z1])\n",
        "    for t in 1:L-1\n",
        "        observations[:zs=>t=>:x] = Observation(Any[:zs=>t=>:z])\n",
        "    end\n",
        "\n",
        "    fg = compile_trace_to_factor_graph(trace, latents, observations)\n",
        "\n",
        "    # FFBS-like elimination order over z's\n",
        "    order = Any[:z1; ([:zs=>t=>:z for t in 1:L-1]...)]\n",
        "\n",
        "    return (fg=fg, order=order, trace=trace, latents=latents, observations=observations)\n",
        "end\n",
        "\n",
        "# identical FG+order, native vs einsum\n",
        "function bench_chain_sml(K::Int, L::Int;\n",
        "        seconds::Float64=1.0, backend::Symbol=:auto,\n",
        "        dtype::Symbol=:float32, jit::Bool=true, cache::Bool=true)\n",
        "\n",
        "    data = make_fg_chain(K, L)\n",
        "    fg, order = data.fg, data.order\n",
        "\n",
        "    # Warm up both engines (important for JAX/XLA and opt_einsum expression caching)\n",
        "    _ = variable_elimination(fg, order; engine=:native)\n",
        "    _ = variable_elimination(fg, order; engine=:einsum,\n",
        "                             backend=backend, dtype=dtype, jit=jit, cache=cache)\n",
        "\n",
        "    tn = @belapsed variable_elimination($fg, $order; engine=:native) seconds=seconds evals=1\n",
        "    te = @belapsed variable_elimination($fg, $order; engine=:einsum,\n",
        "                                        backend=$backend, dtype=$dtype, jit=$jit, cache=$cache) seconds=seconds evals=1\n",
        "    sp = tn / te\n",
        "\n",
        "    println(\"SML chain VE benchmark  (K=$K, L=$L, backend=$(backend), dtype=$(dtype), jit=$(jit))\")\n",
        "    @printf(\"  native (CPU greedy):      %8.4f s\\n\", tn)\n",
        "    @printf(\"  einsum (opt_einsum/JAX):  %8.4f s\\n\", te)\n",
        "    @printf(\"  speedup (native/einsum):  %8.3fx\\n\", sp)\n",
        "\n",
        "    return (tn=tn, te=te, speedup=sp, fg=fg, order=order, extras=data)\n",
        "end\n",
        "\n",
        "bench_chain_sml(16, 32; seconds=1.0, backend=:auto, dtype=:float32, jit=true);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37wUb6Oq0KqU",
        "outputId": "417e0956-396e-4b75-b559-49b4c7ea11c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SML chain VE benchmark  (K=32, L=64, backend=auto, dtype=float32, jit=true)\n",
            "  native (CPU greedy):      752.2822 s\n",
            "  einsum (opt_einsum/JAX):    2.5412 s\n",
            "  speedup (native/einsum):   296.038x\n"
          ]
        }
      ],
      "source": [
        "bench_chain_sml(32, 64; seconds=2.0, backend=:auto, dtype=:float32, jit=true);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUPz4TRKMYNb"
      },
      "source": [
        "## speed check 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tubnRqOoMekY",
        "outputId": "28abc0cb-a8f5-4a60-80a1-0bd383e30212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "map_large        n=50  engine=native  dtype=float32  steps=5  time=51.995 s\n",
            "map_large        n=50  engine=einsum  dtype=float32  steps=5  time=7.982 s\n",
            "einsum speedup: 6.51x (native/einsum)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(native = 51.995298365, einsum = 7.982030794, speedup = 6.514043820036909)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Scaled Map model (overhead sanity)\n",
        "\n",
        "# Reuse the already-defined local_model/global_model if present\n",
        "if !isdefined(Main, :local_model)\n",
        "@gen (static) function local_model(inlier_mean)\n",
        "    z ~ bernoulli(0.5)\n",
        "    x ~ normal(z ? inlier_mean : 0.0, z ? 1.0 : 10.0)\n",
        "end\n",
        "end\n",
        "if !isdefined(Main, :global_model)\n",
        "@gen (static) function global_model(n)\n",
        "    inlier_mean ~ normal(0, 10.0)\n",
        "    data ~ Map(local_model)(fill(inlier_mean, n))\n",
        "end\n",
        "end\n",
        "\n",
        "function bench_map_large(; n, dtype=:float32,\n",
        "                          backend=:auto, optimize=\"auto\",\n",
        "                          jit=true, cache=true)\n",
        "\n",
        "    # inner runner so both engines share the same setup/printing\n",
        "    run_one(engine) = begin\n",
        "        trace = Gen.simulate(global_model, (n,))\n",
        "        sampler = generate_backwards_sampler_fixed_structure(\n",
        "            trace, [(:data=>i=>:z) for i in 1:n];\n",
        "            engine=engine, backend=backend, optimize=optimize,\n",
        "            dtype=dtype, jit=jit, cache=cache\n",
        "        )\n",
        "        steps = 5\n",
        "        t = @elapsed begin\n",
        "            for _ in 1:steps\n",
        "                trace, _ = Gen.mh(trace, sampler, ())\n",
        "                trace, _ = Gen.mh(trace, Gen.select(:inlier_mean))\n",
        "            end\n",
        "        end\n",
        "        println(\"map_large        n=$n  engine=$engine  dtype=$dtype  steps=$steps  time=$(round(t, digits=3)) s\")\n",
        "        t\n",
        "    end\n",
        "\n",
        "    t_native = run_one(:native)\n",
        "    t_einsum = run_one(:einsum)\n",
        "    speedup  = t_native / t_einsum\n",
        "    @printf \"einsum speedup: %.2fx (native/einsum)\\n\" speedup\n",
        "\n",
        "    return (; native=t_native, einsum=t_einsum, speedup=speedup)\n",
        "end\n",
        "\n",
        "bench_map_large(n=50, dtype=:float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNqOZWuCeQMx",
        "outputId": "521d1fa8-7340-48cd-bc35-0ea4bd6421e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "map_large        n=100  engine=native  dtype=float32  steps=5  time=330.215 s\n",
            "map_large        n=100  engine=einsum  dtype=float32  steps=5  time=61.036 s\n",
            "einsum speedup: 5.41x (native/einsum)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(native = 330.214679557, einsum = 61.036224029, speedup = 5.41014266216904)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bench_map_large(n=100, dtype=:float32)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "F8A0HtvgGYLP",
        "tKOVQ7k0tApW",
        "tiA2BnQfzxPP",
        "aSb272U8szI6",
        "kmtI8rgJzxc7",
        "qLAGb1U7bS3_",
        "JVNkKz-ho2KJ",
        "Wse8KLL3zxMe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Julia",
      "name": "julia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
